{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43070928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magistral_finetuning import MagistralFineTuningConfig, MagistralFineTuning, ThinkingMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MagistralFineTuningConfig(\n",
    "    model_name=\"mistralai/Magistral-Small-2506\",\n",
    "    train_file=\"data/mixed_distributed.jsonl\", \n",
    "    output_dir=\"./model/magistral_mixed_v1_fixed\",\n",
    "    thinking_mode=ThinkingMode.MIXED,\n",
    "    batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_epochs=1,\n",
    "    max_length=1024,\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_alternative_trainer(finetuner, train_data):\n",
    "    \"\"\"Alternative trainer setup that avoids SFTTrainer pad token issues\"\"\"\n",
    "    \n",
    "    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Prepare dataset manually\n",
    "    train_dataset = finetuner.prepare_dataset(train_data)\n",
    "    \n",
    "    # Create data collator that handles Magistral tokenizer properly\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=finetuner.tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=None,  # Let it handle padding naturally\n",
    "    )\n",
    "    \n",
    "    # Training arguments (simpler approach)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        num_train_epochs=config.num_epochs,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        gradient_checkpointing=config.gradient_checkpointing,\n",
    "        learning_rate=config.learning_rate,\n",
    "        lr_scheduler_type=config.lr_scheduler_type,\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        seed=42,\n",
    "        bf16=True,\n",
    "        tf32=True,\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        dataloader_num_workers=config.dataloader_num_workers,\n",
    "        dataloader_pin_memory=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Tokenize dataset for standard Trainer\n",
    "    def tokenize_function(examples):\n",
    "        return finetuner.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,  # Let data collator handle padding\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "    \n",
    "    # Create standard trainer\n",
    "    trainer = Trainer(\n",
    "        model=finetuner.model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner = MagistralFineTuning(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7baf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = finetuner.load_jsonl(config.train_file)\n",
    "print(f\"Training samples: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up model...\")\n",
    "finetuner.setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up trainer...\")\n",
    "try:\n",
    "    finetuner.setup_trainer(train_data)\n",
    "    print(\"✅ SFTTrainer setup successful!\")\n",
    "    use_alternative = False\n",
    "except Exception as e:\n",
    "    print(f\"❌ SFTTrainer failed: {e}\")\n",
    "    print(\"Using alternative trainer setup...\")\n",
    "    finetuner.trainer = setup_alternative_trainer(finetuner, train_data)\n",
    "    print(\"✅ Alternative trainer setup successful!\")\n",
    "    use_alternative = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef696816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor memory and start training\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "print(f\"GPU memory reserved: {torch.cuda.memory_reserved()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "finetuner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model...\")\n",
    "finetuner.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadad74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_type = \"Alternative Trainer\" if use_alternative else \"SFTTrainer\"\n",
    "print(f\"\\n✅ Mixed training complete with {trainer_type}!\")\n",
    "print(f\"Model saved to: {config.output_dir}\")\n",
    "print(f\"Training mode: {config.thinking_mode.value}\")\n",
    "print(f\"Effective batch size: {config.effective_batch_size}\")\n",
    "print(f\"Max sequence length: {config.max_length}\")\n",
    "print(f\"Final learning rate: {config.learning_rate}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
