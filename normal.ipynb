{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3738f2db",
      "metadata": {},
      "outputs": [],
      "source": [
        "from magistral_finetuning import MagistralFineTuningConfig, MagistralFineTuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335cc48f",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = MagistralFineTuningConfig(\n",
        "    model_name=\"mistralai/Magistral-Small-2506\",\n",
        "    train_file=\"data/train.jsonl\",\n",
        "    output_dir=\"./model/magistral_normal_v1_fixed\",\n",
        "    batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    num_epochs=1,\n",
        "    max_length=512,\n",
        "    lora_r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    gradient_checkpointing=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3927724",
      "metadata": {},
      "outputs": [],
      "source": [
        "config.print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf5ec67",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative trainer setup function\n",
        "def setup_alternative_trainer(finetuner, train_data):\n",
        "    \"\"\"Alternative trainer setup that avoids SFTTrainer pad token issues\"\"\"\n",
        "    \n",
        "    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "    \n",
        "    # Prepare dataset manually\n",
        "    train_dataset = finetuner.prepare_dataset(train_data)\n",
        "    \n",
        "    # Create data collator that handles Magistral tokenizer properly\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=finetuner.tokenizer,\n",
        "        mlm=False,\n",
        "        pad_to_multiple_of=None,\n",
        "    )\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config.output_dir,\n",
        "        num_train_epochs=config.num_epochs,\n",
        "        per_device_train_batch_size=config.batch_size,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        gradient_checkpointing=config.gradient_checkpointing,\n",
        "        learning_rate=config.learning_rate,\n",
        "        lr_scheduler_type=config.lr_scheduler_type,\n",
        "        warmup_ratio=config.warmup_ratio,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        save_strategy=\"epoch\",\n",
        "        seed=42,\n",
        "        bf16=True,\n",
        "        tf32=True,\n",
        "        optim=\"adamw_bnb_8bit\",\n",
        "        dataloader_num_workers=config.dataloader_num_workers,\n",
        "        dataloader_pin_memory=True,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "    \n",
        "    # Tokenize dataset for standard Trainer\n",
        "    def tokenize_function(examples):\n",
        "        return finetuner.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=config.max_length,\n",
        "            return_tensors=None,\n",
        "        )\n",
        "    \n",
        "    tokenized_dataset = train_dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=train_dataset.column_names,\n",
        "    )\n",
        "    \n",
        "    # Create standard trainer\n",
        "    trainer = Trainer(\n",
        "        model=finetuner.model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef816c99",
      "metadata": {},
      "outputs": [],
      "source": [
        "finetuner = MagistralFineTuning(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bfb53f",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = finetuner.load_jsonl(config.train_file)\n",
        "print(f\"Training samples: {len(train_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfe963b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Setting up model...\")\n",
        "finetuner.setup_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ba0aaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    finetuner.setup_trainer(train_data)\n",
        "    print(\"✅ SFTTrainer setup successful!\")\n",
        "    use_alternative = False\n",
        "except Exception as e:\n",
        "    print(f\"❌ SFTTrainer failed: {e}\")\n",
        "    print(\"Using alternative trainer setup...\")\n",
        "    finetuner.trainer = setup_alternative_trainer(finetuner, train_data)\n",
        "    print(\"✅ Alternative trainer setup successful!\")\n",
        "    use_alternative = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91358653",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "finetuner.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfac076",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Saving model...\")\n",
        "finetuner.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66fdb84f",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_type = \"Alternative Trainer\" if use_alternative else \"SFTTrainer\"\n",
        "print(f\"\\n✅ QLoRA training complete with {trainer_type}!\")\n",
        "print(f\"Model saved to: {config.output_dir}\")\n",
        "print(f\"Total trainable parameters: {finetuner.model.get_nb_trainable_parameters():,}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
